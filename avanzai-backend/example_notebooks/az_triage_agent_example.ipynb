{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GuillermoMalena_1/Desktop/avanzai-open/avanzai-backend/avanzai-backend/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Supabase client initialization failed: Invalid API key\n",
      "Warning: Supabase client initialization failed: Invalid API key\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>CIK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>66740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>91142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1551152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>1467373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>XYL</td>\n",
       "      <td>Xylem Inc.</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Machinery &amp; Supplies &amp; Components</td>\n",
       "      <td>White Plains, New York</td>\n",
       "      <td>1524472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>YUM</td>\n",
       "      <td>Yum! Brands</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Louisville, Kentucky</td>\n",
       "      <td>1041061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ZBRA</td>\n",
       "      <td>Zebra Technologies</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Electronic Equipment &amp; Instruments</td>\n",
       "      <td>Lincolnshire, Illinois</td>\n",
       "      <td>877212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>ZBH</td>\n",
       "      <td>Zimmer Biomet</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>Warsaw, Indiana</td>\n",
       "      <td>1136869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>Parsippany, New Jersey</td>\n",
       "      <td>1555280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol             Security             GICS Sector  \\\n",
       "0      MMM                   3M             Industrials   \n",
       "1      AOS          A. O. Smith             Industrials   \n",
       "2      ABT  Abbott Laboratories             Health Care   \n",
       "3     ABBV               AbbVie             Health Care   \n",
       "4      ACN            Accenture  Information Technology   \n",
       "..     ...                  ...                     ...   \n",
       "498    XYL           Xylem Inc.             Industrials   \n",
       "499    YUM          Yum! Brands  Consumer Discretionary   \n",
       "500   ZBRA   Zebra Technologies  Information Technology   \n",
       "501    ZBH        Zimmer Biomet             Health Care   \n",
       "502    ZTS               Zoetis             Health Care   \n",
       "\n",
       "                                GICS Sub-Industry    Headquarters Location  \\\n",
       "0                        Industrial Conglomerates    Saint Paul, Minnesota   \n",
       "1                               Building Products     Milwaukee, Wisconsin   \n",
       "2                           Health Care Equipment  North Chicago, Illinois   \n",
       "3                                   Biotechnology  North Chicago, Illinois   \n",
       "4                  IT Consulting & Other Services          Dublin, Ireland   \n",
       "..                                            ...                      ...   \n",
       "498  Industrial Machinery & Supplies & Components   White Plains, New York   \n",
       "499                                   Restaurants     Louisville, Kentucky   \n",
       "500            Electronic Equipment & Instruments   Lincolnshire, Illinois   \n",
       "501                         Health Care Equipment          Warsaw, Indiana   \n",
       "502                               Pharmaceuticals   Parsippany, New Jersey   \n",
       "\n",
       "         CIK  \n",
       "0      66740  \n",
       "1      91142  \n",
       "2       1800  \n",
       "3    1551152  \n",
       "4    1467373  \n",
       "..       ...  \n",
       "498  1524472  \n",
       "499  1041061  \n",
       "500   877212  \n",
       "501  1136869  \n",
       "502  1555280  \n",
       "\n",
       "[503 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current notebook's directory\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "# Add the notebook utilities directory to the Python path\n",
    "# Note: Using 'notebook utilities' with a space as you mentioned\n",
    "sys.path.append(os.path.join(current_dir, 'notebook_utilities'))\n",
    "\n",
    "# Import necessary libraries for the financial analysis system\n",
    "from az_main import get_data_requirements, QueryParams, PricingRequest, SessionManager, get_macro_tickers, QueryRequest, get_macro_config\n",
    "from az_data import *\n",
    "from uuid import uuid4\n",
    "from fredapi import Fred\n",
    "\n",
    "# Initialize FRED client\n",
    "FRED_API_KEY = 'ENTER YOUR FRED API KEY HERE'\n",
    "fred = Fred(FRED_API_KEY)\n",
    "import gc\n",
    "\n",
    "# Download/cache the universe\n",
    "db_path = download_sp500_universe()\n",
    "\n",
    "# Query back into a DataFrame\n",
    "sp500_universe_df = query_sp500_universe(db_path)\n",
    "sp500_universe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "az_universe.db\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>name</th>\n",
       "      <th>asset_class</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EURUSD=X</td>\n",
       "      <td>EUR/USD</td>\n",
       "      <td>fx_cross</td>\n",
       "      <td>2025-01-25 12:46:59.125157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JPYUSD=X</td>\n",
       "      <td>JPY/USD</td>\n",
       "      <td>fx_cross</td>\n",
       "      <td>2025-01-25 12:46:59.125182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBPUSD=X</td>\n",
       "      <td>GBP/USD</td>\n",
       "      <td>fx_cross</td>\n",
       "      <td>2025-01-25 12:46:59.125183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CADUSD=X</td>\n",
       "      <td>CAD/USD</td>\n",
       "      <td>fx_cross</td>\n",
       "      <td>2025-01-25 12:46:59.125185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHFUSD=X</td>\n",
       "      <td>CHF/USD</td>\n",
       "      <td>fx_cross</td>\n",
       "      <td>2025-01-25 12:46:59.125187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>EFA</td>\n",
       "      <td>iShares MSCI EAFE ETF</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-01-25 12:48:44.720475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>VT</td>\n",
       "      <td>Vanguard Total World Stock ETF</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-01-25 12:48:44.720475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>SCHF</td>\n",
       "      <td>Schwab International Equity ETF</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-01-25 12:48:44.720475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>IXUS</td>\n",
       "      <td>iShares Core MSCI Total International Stock ETF</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-01-25 12:48:44.720475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>VEU</td>\n",
       "      <td>Vanguard FTSE All-World ex-US ETF</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-01-25 12:48:44.720475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ticker                                             name asset_class  \\\n",
       "0     EURUSD=X                                          EUR/USD    fx_cross   \n",
       "1     JPYUSD=X                                          JPY/USD    fx_cross   \n",
       "2     GBPUSD=X                                          GBP/USD    fx_cross   \n",
       "3     CADUSD=X                                          CAD/USD    fx_cross   \n",
       "4     CHFUSD=X                                          CHF/USD    fx_cross   \n",
       "...        ...                                              ...         ...   \n",
       "1147       EFA                            iShares MSCI EAFE ETF         etf   \n",
       "1148        VT                   Vanguard Total World Stock ETF         etf   \n",
       "1149      SCHF                  Schwab International Equity ETF         etf   \n",
       "1150      IXUS  iShares Core MSCI Total International Stock ETF         etf   \n",
       "1151       VEU                Vanguard FTSE All-World ex-US ETF         etf   \n",
       "\n",
       "                     last_update  \n",
       "0     2025-01-25 12:46:59.125157  \n",
       "1     2025-01-25 12:46:59.125182  \n",
       "2     2025-01-25 12:46:59.125183  \n",
       "3     2025-01-25 12:46:59.125185  \n",
       "4     2025-01-25 12:46:59.125187  \n",
       "...                          ...  \n",
       "1147  2025-01-25 12:48:44.720475  \n",
       "1148  2025-01-25 12:48:44.720475  \n",
       "1149  2025-01-25 12:48:44.720475  \n",
       "1150  2025-01-25 12:48:44.720475  \n",
       "1151  2025-01-25 12:48:44.720475  \n",
       "\n",
       "[1152 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set path to CSV file in notebook_utilities subfolder\n",
    "csv_path = os.path.join('notebook_utilities', 'az_universe_05012025.csv')\n",
    "\n",
    "\n",
    "az_universe_path  = import_csv_universe(csv_path)\n",
    "print(az_universe_path)\n",
    "az_universe = query_az_universe(az_universe_path)\n",
    "az_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1148 of 1148 completed\n",
      "\n",
      "12 Failed downloads:\n",
      "['BRK.B', 'HEI.A', 'CWEN.A', '^SENSEX', 'BF.A']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "['BF.B', 'LEN.B', 'UHAL.B']: YFPricesMissingError('possibly delisted; no price data found  (1d 2010-01-01 -> 2025-05-01)')\n",
      "['HCP', 'TPX', 'SMAR', 'AGR']: YFPricesMissingError('possibly delisted; no price data found  (1d 2010-01-01 -> 2025-05-01) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "/Users/GuillermoMalena_1/Desktop/avanzai-open/avanzai-backend/example_notebooks/notebook_utilities/az_data.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  price_data.reset_index(inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'az_pricing_latest.parquet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parquet_file = download_pricing(csv_path)\n",
    "parquet_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>A</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAON</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABNB</th>\n",
       "      <th>ABT</th>\n",
       "      <th>...</th>\n",
       "      <th>^IRX</th>\n",
       "      <th>^IXIC</th>\n",
       "      <th>^N225</th>\n",
       "      <th>^NSEI</th>\n",
       "      <th>^RUI</th>\n",
       "      <th>^RUT</th>\n",
       "      <th>^SENSEX</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^TYX</th>\n",
       "      <th>^VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>19.973598</td>\n",
       "      <td>35.697647</td>\n",
       "      <td>4.496877</td>\n",
       "      <td>3.479506</td>\n",
       "      <td>35.270969</td>\n",
       "      <td>6.440331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.579693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055</td>\n",
       "      <td>2308.419922</td>\n",
       "      <td>10654.790039</td>\n",
       "      <td>5232.200195</td>\n",
       "      <td>621.890015</td>\n",
       "      <td>640.099976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.841</td>\n",
       "      <td>4.660</td>\n",
       "      <td>20.040001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>19.756628</td>\n",
       "      <td>34.582764</td>\n",
       "      <td>5.005958</td>\n",
       "      <td>3.378549</td>\n",
       "      <td>35.061302</td>\n",
       "      <td>6.451465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.429583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>2308.709961</td>\n",
       "      <td>10681.830078</td>\n",
       "      <td>5277.899902</td>\n",
       "      <td>623.969971</td>\n",
       "      <td>638.489990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.755</td>\n",
       "      <td>4.593</td>\n",
       "      <td>19.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>19.686436</td>\n",
       "      <td>36.383728</td>\n",
       "      <td>4.798553</td>\n",
       "      <td>3.244521</td>\n",
       "      <td>35.367039</td>\n",
       "      <td>6.348845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.531939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045</td>\n",
       "      <td>2301.090088</td>\n",
       "      <td>10731.450195</td>\n",
       "      <td>5281.799805</td>\n",
       "      <td>624.599976</td>\n",
       "      <td>637.950012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.808</td>\n",
       "      <td>4.671</td>\n",
       "      <td>19.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>19.660906</td>\n",
       "      <td>35.611885</td>\n",
       "      <td>4.939965</td>\n",
       "      <td>3.364625</td>\n",
       "      <td>35.358303</td>\n",
       "      <td>6.337109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.685461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045</td>\n",
       "      <td>2300.050049</td>\n",
       "      <td>10681.660156</td>\n",
       "      <td>5263.100098</td>\n",
       "      <td>627.109985</td>\n",
       "      <td>641.969971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.822</td>\n",
       "      <td>4.689</td>\n",
       "      <td>19.059999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>2025-04-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5104</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>106.870003</td>\n",
       "      <td>25.559999</td>\n",
       "      <td>9.850000</td>\n",
       "      <td>88.790001</td>\n",
       "      <td>32.630001</td>\n",
       "      <td>210.139999</td>\n",
       "      <td>192.339996</td>\n",
       "      <td>123.300003</td>\n",
       "      <td>129.529999</td>\n",
       "      <td>...</td>\n",
       "      <td>4.193</td>\n",
       "      <td>17366.130859</td>\n",
       "      <td>35839.988281</td>\n",
       "      <td>24328.500000</td>\n",
       "      <td>3024.310059</td>\n",
       "      <td>1965.550049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.216</td>\n",
       "      <td>4.694</td>\n",
       "      <td>25.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5105</th>\n",
       "      <td>2025-04-29</td>\n",
       "      <td>107.459999</td>\n",
       "      <td>25.190001</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>89.110001</td>\n",
       "      <td>33.310001</td>\n",
       "      <td>211.210007</td>\n",
       "      <td>193.509995</td>\n",
       "      <td>125.489998</td>\n",
       "      <td>130.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.190</td>\n",
       "      <td>17461.320312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24335.949219</td>\n",
       "      <td>3042.159912</td>\n",
       "      <td>1976.520020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.173</td>\n",
       "      <td>4.648</td>\n",
       "      <td>24.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>107.599998</td>\n",
       "      <td>24.530001</td>\n",
       "      <td>9.950000</td>\n",
       "      <td>91.269997</td>\n",
       "      <td>32.720001</td>\n",
       "      <td>212.500000</td>\n",
       "      <td>195.100006</td>\n",
       "      <td>121.919998</td>\n",
       "      <td>130.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.190</td>\n",
       "      <td>17446.339844</td>\n",
       "      <td>36045.378906</td>\n",
       "      <td>24334.199219</td>\n",
       "      <td>3045.669922</td>\n",
       "      <td>1964.119995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.177</td>\n",
       "      <td>4.681</td>\n",
       "      <td>24.700001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5107 rows × 1149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date           A         AA       AAL       AAON        AAP  \\\n",
       "0    2010-01-01         NaN        NaN       NaN        NaN        NaN   \n",
       "1    2010-01-04   19.973598  35.697647  4.496877   3.479506  35.270969   \n",
       "2    2010-01-05   19.756628  34.582764  5.005958   3.378549  35.061302   \n",
       "3    2010-01-06   19.686436  36.383728  4.798553   3.244521  35.367039   \n",
       "4    2010-01-07   19.660906  35.611885  4.939965   3.364625  35.358303   \n",
       "...         ...         ...        ...       ...        ...        ...   \n",
       "5102 2025-04-26         NaN        NaN       NaN        NaN        NaN   \n",
       "5103 2025-04-27         NaN        NaN       NaN        NaN        NaN   \n",
       "5104 2025-04-28  106.870003  25.559999  9.850000  88.790001  32.630001   \n",
       "5105 2025-04-29  107.459999  25.190001  9.900000  89.110001  33.310001   \n",
       "5106 2025-04-30  107.599998  24.530001  9.950000  91.269997  32.720001   \n",
       "\n",
       "            AAPL        ABBV        ABNB         ABT  ...   ^IRX  \\\n",
       "0            NaN         NaN         NaN         NaN  ...    NaN   \n",
       "1       6.440331         NaN         NaN   18.579693  ...  0.055   \n",
       "2       6.451465         NaN         NaN   18.429583  ...  0.060   \n",
       "3       6.348845         NaN         NaN   18.531939  ...  0.045   \n",
       "4       6.337109         NaN         NaN   18.685461  ...  0.045   \n",
       "...          ...         ...         ...         ...  ...    ...   \n",
       "5102         NaN         NaN         NaN         NaN  ...    NaN   \n",
       "5103         NaN         NaN         NaN         NaN  ...    NaN   \n",
       "5104  210.139999  192.339996  123.300003  129.529999  ...  4.193   \n",
       "5105  211.210007  193.509995  125.489998  130.500000  ...  4.190   \n",
       "5106  212.500000  195.100006  121.919998  130.750000  ...  4.190   \n",
       "\n",
       "             ^IXIC         ^N225         ^NSEI         ^RUI         ^RUT  \\\n",
       "0              NaN           NaN           NaN          NaN          NaN   \n",
       "1      2308.419922  10654.790039   5232.200195   621.890015   640.099976   \n",
       "2      2308.709961  10681.830078   5277.899902   623.969971   638.489990   \n",
       "3      2301.090088  10731.450195   5281.799805   624.599976   637.950012   \n",
       "4      2300.050049  10681.660156   5263.100098   627.109985   641.969971   \n",
       "...            ...           ...           ...          ...          ...   \n",
       "5102           NaN           NaN           NaN          NaN          NaN   \n",
       "5103           NaN           NaN           NaN          NaN          NaN   \n",
       "5104  17366.130859  35839.988281  24328.500000  3024.310059  1965.550049   \n",
       "5105  17461.320312           NaN  24335.949219  3042.159912  1976.520020   \n",
       "5106  17446.339844  36045.378906  24334.199219  3045.669922  1964.119995   \n",
       "\n",
       "      ^SENSEX   ^TNX   ^TYX       ^VIX  \n",
       "0         NaN    NaN    NaN        NaN  \n",
       "1         NaN  3.841  4.660  20.040001  \n",
       "2         NaN  3.755  4.593  19.350000  \n",
       "3         NaN  3.808  4.671  19.160000  \n",
       "4         NaN  3.822  4.689  19.059999  \n",
       "...       ...    ...    ...        ...  \n",
       "5102      NaN    NaN    NaN        NaN  \n",
       "5103      NaN    NaN    NaN        NaN  \n",
       "5104      NaN  4.216  4.694  25.150000  \n",
       "5105      NaN  4.173  4.648  24.170000  \n",
       "5106      NaN  4.177  4.681  24.700001  \n",
       "\n",
       "[5107 rows x 1149 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az_pricing = pd.read_parquet('az_pricing_latest.parquet')\n",
    "az_pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed tool examples\n",
      "Completed outer examples\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate examples once (run this only when needed)\n",
    "examples = generate_examples(\n",
    "    db_path=\"az_universe.db\",\n",
    "    table=\"az_universe\",\n",
    "    tool_example_count=10,\n",
    "    outer_example_count=10,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# Save examples to a file\n",
    "examples.save_to_file(\"sql_examples.json\")\n",
    "\n",
    "# Step 2: In your regular workflow, load examples and build agent quickly\n",
    "loaded_examples = GeneratedExamples.load_from_file(\"sql_examples.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tickers(query_result) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract and validate tickers from a query result.\n",
    "    \n",
    "    Args:\n",
    "        query_result: The result from sql_query_agent.query() or a string/list of tickers\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of cleaned ticker symbols\n",
    "    \"\"\"\n",
    "    class ListInput(BaseModel):\n",
    "        data: Union[str, List[Any], Any]\n",
    "\n",
    "        @field_validator('data')\n",
    "        @classmethod\n",
    "        def ensure_list_output(cls, v):\n",
    "            if isinstance(v, list):\n",
    "                return [item.strip() if isinstance(item, str) else item for item in v]\n",
    "            if isinstance(v, str):\n",
    "                if not v:\n",
    "                    return []\n",
    "                if v.startswith('[') and v.endswith(']'):\n",
    "                    try:\n",
    "                        import ast\n",
    "                        return [item.strip() if isinstance(item, str) else item for item in ast.literal_eval(v)]\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        return [item.strip() for item in v[1:-1].split(',')]\n",
    "                return [item.strip() for item in v.split(',')]\n",
    "            return [v]\n",
    "\n",
    "    class ListOutput(BaseModel):\n",
    "        result: List[Any]\n",
    "\n",
    "    # Handle both agent response objects and direct string/list inputs\n",
    "    if hasattr(query_result, 'response'):\n",
    "        data = query_result.response\n",
    "    else:\n",
    "        data = query_result\n",
    "\n",
    "    input_data = ListInput(data=data)\n",
    "    return ListOutput(result=input_data.data).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SQL query agent using pre-generated examples\n",
    "agent, _ = build_sql_query_agent(examples=loaded_examples)\n",
    "def get_tickers_local(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get tickers from a natural language query using a SQL query agent.\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language query string\n",
    "        \n",
    "    Returns:\n",
    "        List of ticker symbols extracted from the query\n",
    "    \"\"\"\n",
    "    tickers = agent.query(query)\n",
    "    return extract_tickers(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs: {'pricing_data', 'macro_data'}\n"
     ]
    }
   ],
   "source": [
    "# Example query to analyze\n",
    "query = \"compare AAPL and inflation since 2015\"\n",
    "\n",
    "# Generate a unique session ID for this analysis\n",
    "session_id = str(uuid4())\n",
    "\n",
    "# Identify data requirements for this query\n",
    "reqs = get_data_requirements(QueryParams(user_query=query))\n",
    "needs = set(reqs.keys())  # {'pricing_data', 'macro_data', ...}\n",
    "print(f\"Needs: {needs}\")\n",
    "\n",
    "# Initialize session manager for data storage\n",
    "session_manager = SessionManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: compare AAPL and inflation since 2015\n",
      "=== Calling Function ===\n",
      "Calling function: custom_sql_query with args: {\"query\":\"compare AAPL and inflation since 2015\"}\n",
      "Got output: ['AAPL']\n",
      "========================\n",
      "\n",
      "['AAPL']\n"
     ]
    }
   ],
   "source": [
    "# If pricing data is required, extract and fetch the relevant tickers\n",
    "if \"pricing_data\" in needs:\n",
    "    pricing_tickers = get_tickers_local(query)\n",
    "    print(pricing_tickers)\n",
    "    if pricing_tickers:\n",
    "        p_resp = get_pricing_data_local(\n",
    "            PricingRequest(mode=\"default\", tickers=pricing_tickers)\n",
    "        )\n",
    "        \n",
    "        # Convert to DataFrame and save\n",
    "        pdf = pd.DataFrame(p_resp[\"data\"])\n",
    "        await session_manager.initialize_session(session_id)\n",
    "        session_manager.save_dataframe(pdf, str(session_id), \"pricing_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this function for getting macro data\n",
    "def get_macro_data_endpoint(request: TickerRequest):\n",
    "    \"\"\"Fetch data from FRED and store in session\"\"\"\n",
    "    try:\n",
    "\n",
    "        \n",
    "        # Fetch data for each ticker\n",
    "        dfs = []\n",
    "        for ticker in request.tickers:\n",
    "            series = fred.get_series(ticker)\n",
    "            df = series.to_frame(name=ticker)\n",
    "            df.index.name = 'Date'\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Combine all data\n",
    "        if dfs:\n",
    "            macro_df = pd.concat(dfs, axis=1).reset_index()\n",
    "            \n",
    "            # Initialize session and save data\n",
    "            session_manager.initialize_session(request.session_id)\n",
    "            if not session_manager.save_dataframe(macro_df, request.session_id, \"macro_raw\"):\n",
    "                raise Exception(\"Failed to save macro data to session storage\")\n",
    "                \n",
    "            return {\"message\": \"Data stored successfully\", \"preview\": macro_df.dropna().head(5)}\n",
    "        else:\n",
    "            raise Exception(\"No data found\")\n",
    "    except Exception as e:\n",
    "        raise Exception(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_macro_config() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load macro configuration from JSON file when needed.\n",
    "    Returns:\n",
    "        Dict containing the macro indicator configuration.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config_path = Path(\"notebook_utilities/macro_indicators.json\")\n",
    "        if not config_path.exists():\n",
    "            raise FileNotFoundError(\"Macro configuration file not found\")\n",
    "\n",
    "        with open(config_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid macro configuration JSON: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading macro configuration: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_macro_tickers(request: QueryRequest):\n",
    "    \"\"\"Extract relevant macro tickers based on user query\"\"\"\n",
    "    \n",
    "    class Indicator(BaseModel):\n",
    "        description: str\n",
    "        ticker: str\n",
    "\n",
    "    class IndicatorsExtraction(BaseModel):\n",
    "        indicators: List[Indicator] = Field(\n",
    "            default_factory=list,\n",
    "            description=\n",
    "            \"List of macroeconomic indicators with their descriptions and tickers\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Get macro config\n",
    "        macro_json = get_macro_config()\n",
    "\n",
    "        # Create agent for extractions\n",
    "        macro_agent = Agent(\n",
    "            name=\"macro_extraction_agent\",\n",
    "            output_type=IndicatorsExtraction,\n",
    "            model=\"gpt-4o-mini\",\n",
    "            instructions=f\"\"\"You are an AI assistant tasked with extracting relevant descriptions and tickers from a JSON dataset based on a user's query. The JSON data contains information about various economic indicators, including their descriptions and ticker symbols.\n",
    "\n",
    "            Use this JSON as reference for your query and focus on the description and ticker to get the relevant tickers\n",
    "            {macro_json}\n",
    "            \n",
    "            Here are some examples to guide you:\n",
    "\n",
    "            User Query: \"What's the correlation of initial jobless claims?\"\n",
    "            Output: {{\"indicators\": [{{\"description\": \"Initial Jobless Claims\", \"ticker\": \"ICSA\"}}]}}\n",
    "\n",
    "            User Query: \"What's the correlation of my portfolio and consumer sentiment?\"\n",
    "            Output: {{\"indicators\": [{{\"description\": \"Consumer Sentiment\", \"ticker\": \"UMCSENT\"}}]}}\n",
    "\n",
    "            User Query: \"How do unemployment and GDP affect inflation?\"\n",
    "            Output: {{\"indicators\": [\n",
    "                {{\"description\": \"Unemployment Rate\", \"ticker\": \"UNRATE\"}},\n",
    "                {{\"description\": \"Gross Domestic Product\", \"ticker\": \"GDP\"}},\n",
    "                {{\"description\": \"Consumer Price Index\", \"ticker\": \"CPIAUCSL\"}}\n",
    "            ]}}\"\"\"\n",
    "        )\n",
    "\n",
    "        # Get response from agent\n",
    "        response = await Runner.run(macro_agent, request.prompt)\n",
    "        indicators = response.final_output.indicators\n",
    "        tickers = [indicator.ticker for indicator in indicators]\n",
    "        return {\"macro_tickers\": tickers}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_macro_tickers_endpoint: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this function for making DataFrames JSON-serializable\n",
    "import re\n",
    "def make_dataframe_json_serializable(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Convert DataFrame to JSON serializable format.\"\"\"\n",
    "    # Identify columns that contain 'date' (case-insensitive)\n",
    "    date_columns = [\n",
    "        col for col in df.columns if re.search('date', col, re.IGNORECASE)\n",
    "    ]\n",
    "\n",
    "    # Convert identified date columns to string\n",
    "    df[date_columns] = df[date_columns].astype(str)\n",
    "\n",
    "    # Check if the index is a datetime type and convert to string if necessary\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = df.index.astype(str)\n",
    "\n",
    "    # Convert to JSON-serializable format\n",
    "    json_serializable_dict = df.replace({\n",
    "        np.nan: None\n",
    "    }).head(5).to_dict(orient='records')\n",
    "\n",
    "    return json_serializable_dict\n",
    "\n",
    "# Add this function for getting macro data\n",
    "def get_macro_data_endpoint(request: TickerRequest):\n",
    "    \"\"\"Fetch data from FRED and store in session\"\"\"\n",
    "    try:\n",
    "        # Initialize FRED client\n",
    "        # Fetch data for each ticker\n",
    "        dfs = []\n",
    "        for ticker in request.tickers:\n",
    "            series = fred.get_series(ticker)\n",
    "            df = series.to_frame(name=ticker)\n",
    "            df.index.name = 'Date'\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Combine all data\n",
    "        if dfs:\n",
    "            macro_df = pd.concat(dfs, axis=1).reset_index()\n",
    "            result = make_dataframe_json_serializable(macro_df)\n",
    "            \n",
    "            # Initialize session and save data\n",
    "            session_manager.initialize_session(request.session_id)\n",
    "            if not session_manager.save_dataframe(macro_df, request.session_id, \"macro_raw\"):\n",
    "                raise Exception(\"Failed to save macro data to session storage\")\n",
    "                \n",
    "            return {\"message\": \"Data stored successfully\", \"preview\": macro_df.dropna().head(5)}\n",
    "        else:\n",
    "            raise Exception(\"No data found\")\n",
    "    except Exception as e:\n",
    "        raise Exception(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/ysgt6z6x0_373k9jgrg82fzh0000gn/T/ipykernel_35980/1525715103.py:43: RuntimeWarning: coroutine 'SessionManager.initialize_session' was never awaited\n",
      "  session_manager.initialize_session(request.session_id)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Initialize list for macro tickers\n",
    "macro_tickers = []\n",
    "\n",
    "# If macro data is required, fetch the relevant indicators\n",
    "if \"macro_data\" in needs:\n",
    "    # Get macro tickers using the function\n",
    "    mp = await get_macro_tickers(\n",
    "        QueryRequest(prompt=query, session_id=str(session_id))\n",
    "    )\n",
    "    mp_tickers = mp[\"macro_tickers\"]\n",
    "    \n",
    "    # Validate tickers against the config\n",
    "    macro_config = get_macro_config()\n",
    "    valid_series = [ind[\"ticker\"] for ind in macro_config[\"indicators\"]]\n",
    "    macro_tickers = [t for t in mp_tickers if t in valid_series]\n",
    "    \n",
    "    # Get and store the macro data\n",
    "    if macro_tickers:\n",
    "        get_macro_data_endpoint(\n",
    "            TickerRequest(tickers=macro_tickers, session_id=str(session_id))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for the agent system\n",
    "from agents import handoff, RunContextWrapper\n",
    "\n",
    "# Model for passing session data between agents\n",
    "class SessionData(BaseModel):\n",
    "    \"\"\"Model for passing session data between agents.\"\"\"\n",
    "    session_id: str\n",
    "    pricing_tickers: list[str]\n",
    "    macro_tickers: list[str]\n",
    "    fundamental_fields: Optional[list[str]]\n",
    "    data_id: Optional[str]\n",
    "\n",
    "# Set up registry for transformation tools\n",
    "TRANSFORM_REGISTRY = []\n",
    "\n",
    "# Decorator to register tools with their data requirements\n",
    "def register_tool(*requires: str):\n",
    "    \"\"\"Decorator that tags a function with its dataset needs.\"\"\"\n",
    "    def wrapper(fn):\n",
    "        fn.requires = set(requires)\n",
    "        fn = function_tool(fn)\n",
    "        fn.requires = set(requires)\n",
    "        TRANSFORM_REGISTRY.append(fn)\n",
    "        return fn\n",
    "    return wrapper\n",
    "\n",
    "# Helper function for filtering dataframes by date range\n",
    "def _filter_by_date(df: pd.DataFrame,\n",
    "                    date_col: str,\n",
    "                    start: Union[str, None],\n",
    "                    end: Union[str, None]) -> pd.DataFrame:\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    if start:\n",
    "        df = df[df[date_col] >= pd.to_datetime(start)]\n",
    "    if end:\n",
    "        df = df[df[date_col] <= pd.to_datetime(end)]\n",
    "    if len(df) < 2:\n",
    "        raise ValueError(\"Insufficient data after date filter\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# 🅰  Pricing-only (already have cumulative_performance, keep it!)\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "@register_tool(\"pricing\")\n",
    "def cumulative_performance(session_id: str,\n",
    "                           tickers: list[str],\n",
    "                           start_date: Union[str, None] = None,\n",
    "                           end_date: Union[str, None] = None) -> dict:    \n",
    "    \"\"\"\n",
    "    Return a dict: {ticker: final cumulative performance value}.\n",
    "    Expects pricing_data.parquet to be in sessions/<session_id>/ .\n",
    "    Stores full cumulative performance series in sessions/<session_id>/cumulative_perf.parquet.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(Path(\"sessions\") / session_id / \"pricing_data.parquet\")\n",
    "    df = _filter_by_date(df, \"date\", start_date, end_date)\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    results_df = pd.DataFrame({\"date\": df[\"date\"]})\n",
    "    \n",
    "    final_results = {}\n",
    "    for t in tickers:\n",
    "        price = df[t].astype(float).ffill()\n",
    "        cr = 100 * (1 + price.pct_change().fillna(0)).cumprod()\n",
    "        results_df[t] = cr.round(2)\n",
    "        # Store the final value for return\n",
    "        final_results[t] = float(cr.iloc[-1].round(2))\n",
    "    \n",
    "    # Save the full cumulative performance to a parquet file\n",
    "    output_path = Path(\"sessions\") / session_id / f\"cumulative_perf_{'-'.join(tickers)}.parquet\"\n",
    "    results_df.to_parquet(output_path)\n",
    "    \n",
    "    # ⬇️ inside cumulative_performance\n",
    "    output_path = Path(\"sessions\") / session_id / f\"cumulative_perf_{'-'.join(tickers)}.parquet\"\n",
    "    results_df.to_parquet(output_path)\n",
    "\n",
    "    return {\n",
    "        \"path\": str(output_path),\n",
    "        \"final_values\": final_results\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 🅱  Macro-only tools\n",
    "# ------------------------------------------------------------------------------------\n",
    "@register_tool(\"macro\")\n",
    "def macro_yoy(session_id: str, ticker: str,\n",
    "                start_date: Union[str, None] = None,\n",
    "                end_date: Union[str, None] = None) -> dict:    \n",
    "    \"\"\"\n",
    "    Year-over-Year % change for a monthly/quarterly FRED series.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(Path(\"sessions\") / session_id / \"macro_raw.parquet\")\n",
    "    df   = _filter_by_date(df, \"Date\", start_date, end_date)\n",
    "    if ticker not in df.columns:\n",
    "        raise ValueError(f\"{ticker} not in macro_raw\")\n",
    "    ser = df.set_index(\"Date\")[ticker].astype(float)\n",
    "    yoy = ser.pct_change(periods=12).dropna() * 100\n",
    "    \n",
    "    # Store the full YoY data to a parquet file\n",
    "    output_path = Path(\"sessions\") / session_id / f\"{ticker}_yoy.parquet\"\n",
    "    yoy_df = pd.DataFrame({ticker+\"_YoY\": yoy})\n",
    "    yoy_df.to_parquet(output_path)\n",
    "    \n",
    "    # Only return the last 20 results\n",
    "    last_20_yoy = yoy.iloc[-20:]\n",
    "    # ⬇️ inside macro_yoy\n",
    "    output_path = Path(\"sessions\") / session_id / f\"{ticker}_yoy.parquet\"\n",
    "    yoy_df.to_parquet(output_path)\n",
    "\n",
    "    last_20 = yoy.iloc[-20:]\n",
    "    return {\n",
    "        \"path\": str(output_path),        # <── add this line\n",
    "        \"preview\": {\n",
    "            \"date\": last_20.index.astype(str).tolist(),\n",
    "            ticker+\"_YoY\": last_20.round(2).tolist()\n",
    "        }\n",
    "    }\n",
    "\n",
    "@register_tool(\"macro\")\n",
    "def macro_level(session_id: str, ticker: str,\n",
    "                start_date: Union[str, None] = None,\n",
    "                end_date: Union[str, None] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Return the raw level of a macro series (e.g., Fed Funds Rate).\n",
    "    Saves <ticker>_level.parquet in the session folder and\n",
    "    returns a small preview + path.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(Path(\"sessions\") / session_id / \"macro_raw.parquet\")\n",
    "    df   = _filter_by_date(df, \"Date\", start_date, end_date)\n",
    "    if ticker not in df.columns:\n",
    "        raise ValueError(f\"{ticker} not in macro_raw\")\n",
    "\n",
    "    ser = df.set_index(\"Date\")[ticker].astype(float)\n",
    "    ser = ser.ffill()\n",
    "    ser = ser.dropna()\n",
    "\n",
    "    # Save full level series\n",
    "    out_path = Path(\"sessions\") / session_id / f\"{ticker}_level.parquet\"\n",
    "    ser.to_frame(name=ticker).to_parquet(out_path)\n",
    "\n",
    "    # Return last 20 points as a preview + the file path\n",
    "    preview = ser.tail(20)\n",
    "    return {\n",
    "        \"path\": str(out_path),\n",
    "        \"date\": preview.index.astype(str).tolist(),\n",
    "        ticker: preview.round(4).tolist()\n",
    "    }\n",
    "\n",
    "@register_tool(\"macro\")\n",
    "def macro_qoq(session_id: str, ticker: str,\n",
    "              start_date: Union[str, None] = None,\n",
    "              end_date: Union[str, None] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Quarter-over-Quarter % change (for GDP).\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(Path(\"sessions\") / session_id / \"macro_raw.parquet\")\n",
    "    df   = _filter_by_date(df, \"Date\", start_date, end_date)\n",
    "    ser = df.set_index(\"Date\")[ticker].astype(float)\n",
    "    qoq = ser.pct_change(periods=4).dropna() * 100\n",
    "    return {\"date\": qoq.index.astype(str).tolist(), ticker+\"_QoQ\": qoq.round(2).tolist()}\n",
    "\n",
    "@register_tool(\"macro\")\n",
    "def macro_auto(session_id: str, ticker: str,\n",
    "               start_date: Union[str, None] = None,\n",
    "               end_date: Union[str, None] = None) -> dict:\n",
    "    macro_config =  get_macro_config()\n",
    "    cfg   = macro_config[ticker][\"default_transform\"]   # 'level' | 'yoy' | 'qoq'\n",
    "    if cfg == \"level\":\n",
    "        return macro_level(session_id, ticker, start_date, end_date)\n",
    "    elif cfg == \"yoy\":\n",
    "        return macro_yoy(session_id, ticker)\n",
    "    elif cfg == \"qoq\":\n",
    "        return macro_qoq(session_id, ticker)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown default transform {cfg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# 🅲  Mixed (pricing + macro) tools\n",
    "# ------------------------------------------------------------------------------------\n",
    "@register_tool(\"pricing\", \"macro\")\n",
    "def join_and_normalise(session_id: str,\n",
    "                       price_ticker: str,\n",
    "                       macro_ticker: str,\n",
    "                       price_base: float ) -> dict:\n",
    "    \"\"\"\n",
    "    Join one price series & one macro series, normalise the price to `price_base`,\n",
    "    forward-fill the macro, and return a combined dataframe.\n",
    "    \"\"\"\n",
    "    # price\n",
    "    pdf = pd.read_parquet(Path(\"sessions\") / session_id / \"pricing_data.parquet\")\n",
    "    pdf[\"date\"] = pd.to_datetime(pdf[\"date\"])\n",
    "    pser = pdf.set_index(\"date\")[price_ticker].astype(float)\n",
    "    pnorm = price_base * (pser / pser.iloc[0])\n",
    "\n",
    "    # macro\n",
    "    mdf  = pd.read_parquet(Path(\"sessions\") / session_id / \"macro_raw.parquet\")\n",
    "    mdf[\"Date\"] = pd.to_datetime(mdf[\"Date\"])\n",
    "    mser = mdf.set_index(\"Date\")[macro_ticker].astype(float).ffill()\n",
    "\n",
    "    # combine\n",
    "    joined = pd.concat([pnorm, mser], axis=1, join=\"inner\").dropna()\n",
    "    joined.columns = [price_ticker+\"_Norm\", macro_ticker]\n",
    "    out = joined.reset_index()\n",
    "    out[\"date\"] = out[\"index\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    out = out.drop(columns=\"index\")\n",
    "    return out.head(10).to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use transform tools from analysis_tools\n",
    "pricing_tools = [t for t in TRANSFORM_REGISTRY if t.requires == {\"pricing\"}]\n",
    "macro_tools   = [t for t in TRANSFORM_REGISTRY if t.requires == {\"macro\"}]\n",
    "mixed_tools   = [t for t in TRANSFORM_REGISTRY if t.requires == {\"pricing\", \"macro\"}]\n",
    "fundamental_tools   = [t for t in TRANSFORM_REGISTRY if t.requires == {\"fundamental\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the specialist agents\n",
    "pricing_agent = Agent(\n",
    "    name=\"pricing_agent\",\n",
    "    tools=pricing_tools,\n",
    "    instructions=\"You have only pricing data in scope. \"\n",
    "                \"Answer with those tools. \"\n",
    "                \"You need to use the session_id provided by the triage agent.\"\n",
    ")\n",
    "\n",
    "macro_agent = Agent(\n",
    "    name=\"macro_agent\",\n",
    "    tools=macro_tools,\n",
    "    instructions=\"You have only macro data in scope. \"\n",
    "                \"Answer with those tools. \"\n",
    "                \"You need to use the session_id provided by the triage agent.\"\n",
    ")\n",
    "\n",
    "mixed_agent = Agent(\n",
    "    name=\"mixed_agent\",\n",
    "    tools=pricing_tools + macro_tools + mixed_tools,\n",
    "    instructions=\"Both pricing and macro data are in scope. \"\n",
    "                \"You may call any tool given to you. \"\n",
    "                \"You need to use the session_id provided by the triage agent.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add handoff handler function\n",
    "async def agent_handoff(ctx: RunContextWrapper[None], input_data: SessionData):\n",
    "    \"\"\"Handle agent handoffs between triage and specialist agents.\"\"\"\n",
    "    print(f\"Agent called with session_id: {input_data.session_id}\")\n",
    "    print(f\"Pricing tickers: {input_data.pricing_tickers}\")\n",
    "    print(f\"Macro tickers: {input_data.macro_tickers}\")\n",
    "    print(f\"Fundamental fields: {input_data.fundamental_fields}\")\n",
    "    print(f\"Data ID: {input_data.data_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add triage instructions generator\n",
    "def triage_instructions(session_id: str, pricing_tickers: list[str], macro_tickers: list[str], fundamental_fields: Optional[list[str]] = None, data_id: Optional[str] = None) -> str:\n",
    "    \"\"\"Generate instructions for the triage agent.\"\"\"\n",
    "    return f\"\"\"\n",
    "    Session ID: {session_id}\n",
    "    Pricing Tickers: {', '.join(pricing_tickers) if pricing_tickers else 'None'}\n",
    "    Macro Tickers: {', '.join(macro_tickers) if macro_tickers else 'None'}\n",
    "    Fundamental Fields: {', '.join(fundamental_fields) if fundamental_fields else 'None'}\n",
    "    Data ID: {data_id if data_id else 'None'}\n",
    "\n",
    "    today's date = f\"{datetime.now().strftime('%Y-%m-%d')}\"\n",
    "    Use today's date to answer any questions about the current date or relative dates.\n",
    "    If the query needs **only pricing data** handoff to pricing_agent.\n",
    "    Examples of pricing data queries:\n",
    "    - \"What's the performance of Meta and Apple since last year?\"\n",
    "    - \"Show me the stock price history of AAPL\"\n",
    "    - \"Calculate the correlation between MSFT and GOOGL\"\n",
    "\n",
    "    If it needs **only macro data** handoff to macro_agent.\n",
    "    Examples of macro data queries:\n",
    "    - \"What was the inflation rate in 2022?\"\n",
    "    - \"Show me unemployment data for the last 5 years\"\n",
    "    - \"Plot the Fed interest rates since 2018\"\n",
    "\n",
    "    If it needs **only fundamental data** handoff to fundamental_agent.\n",
    "    Examples of fundamental data queries:\n",
    "    - \"What was the revenue growth of Apple in the last quarter?\"\n",
    "    - \"Show me the profit margins for Microsoft over the past year\"\n",
    "    - \"Compare the EPS of Google and Amazon\"\n",
    "    - \"What's the debt-to-equity ratio for Tesla?\"\n",
    "\n",
    "    If it needs **both pricing and macro data**, handoff to mixed_agent.\n",
    "    Examples of mixed queries:\n",
    "    - \"Compare S&P 500 performance with inflation rates\"\n",
    "    - \"How does the unemployment rate affect tech stocks?\"\n",
    "    - \"Plot inflation and gold correlation since 2018\"\n",
    "\n",
    "    Always pass the session_id to the agent you handoff to.\n",
    "    For fundamental data queries, always pass the data_id to the fundamental_agent.\n",
    "    Always answer with specific data from the functions you call.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create triage agent with handoffs to specialist agents\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=triage_instructions(str(session_id), pricing_tickers, macro_tickers),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    handoffs=[\n",
    "        handoff(\n",
    "            agent=pricing_agent,\n",
    "            on_handoff=agent_handoff,\n",
    "            input_type=SessionData\n",
    "        ),\n",
    "        handoff(\n",
    "            agent=macro_agent,\n",
    "            on_handoff=agent_handoff,\n",
    "            input_type=SessionData\n",
    "        ),\n",
    "        handoff(\n",
    "            agent=mixed_agent,\n",
    "            on_handoff=agent_handoff,\n",
    "            input_type=SessionData\n",
    "        ),\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent called with session_id: b0c40642-5d96-429f-a66b-2ee9919bbc7b\n",
      "Pricing tickers: ['AAPL']\n",
      "Macro tickers: ['CPIAUCSL']\n",
      "Fundamental fields: None\n",
      "Data ID: None\n",
      "### AAPL Performance Since 2015\n",
      "- **Cumulative Performance**: AAPL has gained approximately 873.75% since 2015.\n",
      "\n",
      "### Inflation (CPI) Details\n",
      "Here are some recent Year-over-Year (YoY) inflation rates based on the Consumer Price Index (CPI):\n",
      "\n",
      "| Date       | YoY Inflation (%) |\n",
      "|------------|-------------------|\n",
      "| Aug 2023   | 3.72              |\n",
      "| Sep 2023   | 3.7               |\n",
      "| Oct 2023   | 3.25              |\n",
      "| Nov 2023   | 3.14              |\n",
      "| Dec 2023   | 3.32              |\n",
      "| Jan 2024   | 3.11              |\n",
      "| Feb 2024   | 3.17              |\n",
      "| Mar 2024   | 3.47              |\n",
      "| Apr 2024   | 3.35              |\n",
      "| May 2024   | 3.24              |\n",
      "\n",
      "AAPL has significantly outperformed inflation over this period. If you have any specific questions or need further analysis, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# 5. Run the triage agent on the query\n",
    "answer = await Runner.run(triage_agent, query)\n",
    "print(answer.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avanzai-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
